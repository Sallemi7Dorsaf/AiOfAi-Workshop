{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qgkQ3HlFC97M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip3 install transformers"],"metadata":{"id":"1D6-iGuADEkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMaFJHD1C5W6"},"outputs":[],"source":["import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFSjX6AIC5W8"},"outputs":[],"source":["train = pd.read_csv('/content/drive/MyDrive/NLP Project/Datasets/Kaggle/trainkaggle.csv')\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"py3DF1XqC5W9"},"outputs":[],"source":["test = pd.read_csv('/content/drive/MyDrive/NLP Project/Datasets/Kaggle/testkaggle.csv')\n","test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIs7S7W7C5W9"},"outputs":[],"source":["#encode output\n","label_mapping = {'fake': 1, 'real': 0}\n","train.label = train.label.map(label_mapping)\n","test.label = test.label.map(label_mapping)"]},{"cell_type":"markdown","metadata":{"id":"TNfjE8aGC5W9"},"source":["### Train val split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvmzkXWqC5W9"},"outputs":[],"source":["train = train.dropna()\n","test = test.dropna()\n","X_train, X_val, y_train, y_val = train_test_split(train['tweetText'], train['label'], test_size=0.2, random_state=42)\n","X_test, y_test = test['tweetText'], test.label"]},{"cell_type":"markdown","metadata":{"id":"mwnsMwTSC5W9"},"source":["#### Preprocessing the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iswcQaB7C5W-"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained('roberta-base')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuDtaQGvC5W-"},"outputs":[],"source":["X_train_encoded = tokenizer(\n","    list(X_train.values),\n","    padding=True,\n","    truncation=True,\n","    return_tensors='pt'\n",")\n","X_val_encoded = tokenizer(\n","    list(X_val.values),\n","    padding=True,\n","    truncation=True,\n","    return_tensors='pt'\n",")\n","X_test_encoded = tokenizer(\n","    list(X_test),\n","    padding=True,\n","    truncation=True,\n","    return_tensors='pt'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTBw5DCwC5W-"},"outputs":[],"source":["class FakeNewsDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zcxTjMiC5W-"},"outputs":[],"source":["train_dataset = FakeNewsDataset(X_train_encoded, y_train.values)\n","val_dataset = FakeNewsDataset(X_val_encoded, y_val.values)\n","test_dataset = FakeNewsDataset(X_test_encoded, y_test.values)\n","train_loader = DataLoader(train_dataset, batch_size=32)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","test_loader = DataLoader(test_dataset, batch_size=32)"]},{"cell_type":"markdown","metadata":{"id":"CwKxU-CGC5W-"},"source":["## Training"]},{"cell_type":"code","source":["pip install evaluate"],"metadata":{"id":"IByCXSZlElvQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7Vodbj9C5W_"},"outputs":[],"source":["from transformers import Trainer, TrainingArguments, RobertaForSequenceClassification\n","import evaluate\n","import numpy as np\n","import os\n","os.environ['HF_MLFLOW_LOG_ARTIFACTS'] = \"1\" # save models as artifact for the expirment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l74L4Q4TC5W_"},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMEywybXC5W_"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='/content/drive/MyDrive/NLP Project/saved_models/roberta/',          # output directory\n","    num_train_epochs=5,              # total number of training epochs\n","    per_device_train_batch_size=8,  # batch size per device during training\n","    per_device_eval_batch_size=8,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=400,\n","    evaluation_strategy='steps',\n","    eval_steps=400,\n","    load_best_model_at_end=True,\n","    save_total_limit=3,\n","    save_steps=400\n","\n",")\n","\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset,             # evaluation dataset\n","    compute_metrics=compute_metrics\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9E7EvwxnC5W_"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"X5n-nuC6C5XA"},"source":["### Calculate performance"]},{"cell_type":"code","source":["eval_result = trainer.evaluate(eval_dataset=val_dataset)\n","\n","# Print the evaluation results\n","print(\"Evaluation results:\")\n","for key, value in eval_result.items():\n","    print(f\"{key}: {value:.4f}\")\n"],"metadata":{"id":"j5JXdxjz5vUP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_result = trainer.evaluate(eval_dataset=test_dataset)\n","\n","# Print the evaluation results\n","print(\"Evaluation results:\")\n","for key, value in eval_result.items():\n","    print(f\"{key}: {value:.4f}\")\n"],"metadata":{"id":"MH2tW_-p6biN"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"nlp_project","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a7481d9fdbdb5bc3cfa1c41a158e56f79c2803b01a0eeac01b32455c6569565e"}},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","toc_visible":true},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}